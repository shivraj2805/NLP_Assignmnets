{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d4cf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected entities (text -> label):\n",
      "- Indian -> NORP\n",
      "- Narendra Modi -> PERSON\n",
      "- Tesla -> PERSON\n",
      "- Elon Musk -> PERSON\n",
      "- New York -> GPE\n",
      "- Tuesday -> DATE\n",
      "- India -> GPE\n",
      "- United Nations -> ORG\n",
      "- the Global Sustainability Summit -> ORG\n",
      "- 2024 -> DATE\n",
      "- Elon Musk -> PERSON\n",
      "- Tesla -> ORG\n",
      "- Asia -> LOC\n",
      "- Gujarat -> GPE\n",
      "- Maharashtra -> NORP\n",
      "- $5 billion -> MONEY\n",
      "- the next five years -> DATE\n",
      "- Google -> ORG\n",
      "- Microsoft -> ORG\n",
      "- AI -> GPE\n",
      "- Bengaluru -> GPE\n",
      "- more than 10,000 -> CARDINAL\n",
      "- 2026 -> DATE\n",
      "\n",
      " Warning: true_labels and predicted_labels lengths differ.\n",
      "true_labels=5, predicted_labels=23\n",
      "Comparing first 5 items only. (Adjust true_labels if you want full match.)\n",
      "\n",
      "Metrics (entity-level):\n",
      "Accuracy : 0.4\n",
      "Precision: 0.13\n",
      "Recall   : 0.2\n",
      "F1 Score : 0.16\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def load_spacy_model():\n",
    "    try:\n",
    "        import spacy\n",
    "        return spacy.load(\"en_core_web_sm\")\n",
    "    except Exception as e:\n",
    "        msg = (\n",
    "            \"Could not load spaCy model 'en_core_web_sm'.\\n\"\n",
    "            \"Fix (run once in terminal):\\n\"\n",
    "            \"  python -m pip install spacy\\n\"\n",
    "            \"  python -m spacy download en_core_web_sm\\n\\n\"\n",
    "            f\"Original error: {e}\"\n",
    "        )\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    nlp = load_spacy_model()\n",
    "\n",
    "    text = \"\"\"\n",
    "Indian Prime Minister Narendra Modi met Tesla CEO Elon Musk in New York on Tuesday\n",
    "to discuss electric vehicle investments in India. The meeting took place at the\n",
    "United Nations headquarters during the Global Sustainability Summit 2024.\n",
    "Later, Elon Musk tweeted that Tesla is planning to expand its operations in Asia,\n",
    "including manufacturing units in Gujarat and Maharashtra. According to sources,\n",
    "the investment could exceed $5 billion over the next five years. Meanwhile, Google\n",
    "and Microsoft also announced new AI research centers in Bengaluru, aiming to hire\n",
    "more than 10,000 engineers by 2026.\n",
    "\"\"\".strip()\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    print(\"Detected entities (text -> label):\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"- {ent.text} -> {ent.label_}\")\n",
    "\n",
    "    predicted_labels = [ent.label_ for ent in doc.ents]\n",
    "\n",
    "    true_labels = [\"ORG\", \"PERSON\", \"PERSON\", \"GPE\", \"DATE\"]\n",
    "\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        m = min(len(true_labels), len(predicted_labels))\n",
    "        print(\"\\n Warning: true_labels and predicted_labels lengths differ.\")\n",
    "        print(f\"true_labels={len(true_labels)}, predicted_labels={len(predicted_labels)}\")\n",
    "        print(f\"Comparing first {m} items only. (Adjust true_labels if you want full match.)\\n\")\n",
    "        true_eval = true_labels[:m]\n",
    "        pred_eval = predicted_labels[:m]\n",
    "    else:\n",
    "        true_eval = true_labels\n",
    "        pred_eval = predicted_labels\n",
    "\n",
    "    accuracy = accuracy_score(true_eval, pred_eval)\n",
    "    precision = precision_score(true_eval, pred_eval, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(true_eval, pred_eval, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(true_eval, pred_eval, average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(\"Metrics (entity-level):\")\n",
    "    print(\"Accuracy :\", round(accuracy, 2))\n",
    "    print(\"Precision:\", round(precision, 2))\n",
    "    print(\"Recall   :\", round(recall, 2))\n",
    "    print(\"F1 Score :\", round(f1, 2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
